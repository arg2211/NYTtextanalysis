scale="response", line = list(col = c("cyan3", "purple3", "red", "blue", "gray55")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional",
line = list(col = c("cyan3", "purple3", "red", "blue", "gray55")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional", alpha = .90,
line = list(col = c("cyan3", "purple3", "red", "blue", "gray55")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional", alpha = .90,
line = list(col = c("cyan3", "purple3")))
visreg(logit2, xvar="bdiscr", by="bmotiv", overlay=T, partial=F, legend=F,
xlab="Ineq. Due to Discrimination", ylab="Ineq. Due to Lack of Motivation",
scale="response", type = "conditional", alpha = .90,
line = list(col = c("cyan3", "purple3", "gray55")))
legend("bottomleft", lwd = 2, c("1", "2", "3", "4", "5"), col = c("cyan3", "purple3", "red", "blue", "gray55"), cex = 0.8)
library(plm)
library(QMSS)
d=read.csv(file.choose()) ## choose "panel-for-R.csv" and more information on variables are here: http://sda.berkeley.edu/sdaweb/analysis/?dataset=gss06panelw3 ##
vars <- c("idnum","panelwave","affrmact","race", "intrace1")
pd.sub <- d[, vars]
pd.sub$black = ifelse(pd.sub$race==2, 1, 0)
pd.sub$intblack = ifelse(pd.sub$intrace1==2, 1, 0)
pd.sub$r.affact = 5-pd.sub$affrmact
pd.sub$year= ifelse(pd.sub$panelwave==3, 1, 0)
lm1 <- lm(r.affact ~ black + intblack + as.factor(panelwave),  data = pd.sub)
summary(lm1)
clusterSE(fit = lm1, cluster.var = "idnum", data=pd.sub)
plm1 <- plm(r.affact ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
summary(plm1)
clusterSE(fit = plm1, cluster.var = "idnum", data=pd.sub)
pd.sub$fourvsall= ifelse(pd.sub$r.affact==4, 1, 0)
pd.sub$fourthreevsall= ifelse(pd.sub$r.affact>=3, 1, 0)
pd.sub$fourthreetwovsone= ifelse(pd.sub$r.affact>=2, 1, 0)
plm2 <- plm(fourvsall ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
plm3 <- plm(fourthreevsall ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
plm4 <- plm(fourthreetwovsone ~ black + intblack + year,  index = c("idnum", "panelwave"),  model = "fd", data = pd.sub)
summary(plm4)
library(stargazer)
stargazer(plm1, plm2, plm3, plm4, type = "text")
pd.sub$d.intblack = firstD(intblack, idnum, pd.sub )
table(pd.sub$d.intblack)
pd.sub$bw=ifelse(pd.sub$d.intblack==-1,1,0)
pd.sub$wb=ifelse(pd.sub$d.intblack==1,1,0)
pd.sub$d.r.affact=firstD(r.affact, idnum, pd.sub )
summary(lm(intblack ~ black, pd.sub))
pd.sub$d.black = firstD(black, idnum, pd.sub )
table(pd.sub$d.black)
summary(lm(d.r.affact ~ bw, pd.sub, subset=black==0))
vars2 = c("colsci", "colscinm", "advfront", "scibnfts", "natsci", "educ", "relig", "fund", "idnum", "panelwave", "id", "year")
v = pan[,vars2]
table(v$panelwave, v$advfront)
prop.table(table(v$panelwave, v$advfront))
table(v$panelwave, v$natsci)
prop.table(table(v$panelwave, v$natsci))
library(plyr)
library(devtools)
library(QMSS)
v$scinec = 5-v$advfront
lm2 <- lm(natsci ~ scinec + as.factor(panelwave), v)
summary(lm2)
sci.pool2 <- plm(natsci ~ scinec + as.factor(panelwave), index = c("idnum", "panelwave"), model = "pooling", data = v)
summary(sci.pool2)
clusterSE(fit = lm2, cluster.var = "idnum", data = v)
pan=read.csv(file.choose()) ## choose panel-for-R.csv
vars2 = c("colsci", "colscinm", "advfront", "scibnfts", "natsci", "educ", "relig", "fund", "idnum", "panelwave", "id", "year")
v = pan[,vars2]
table(v$panelwave, v$advfront)
prop.table(table(v$panelwave, v$advfront))
library(plyr)
library(devtools)
library(QMSS)
v$scinec = 5-v$advfront
lm2 <- lm(natsci ~ scinec + as.factor(panelwave), v)
summary(lm2)
clusterSE(fit = lm2, cluster.var = "idnum", data = v)
v <- ddply(v, "idnum", mutate, d.scinec = firstD(scinec), d.natsci = firstD(natsci))
table(v$d.scinec)
prop.table(table(v$d.scinec))
table(v$d.natsci)
prop.table(table(v$d.natsci))
plm2 <- plm(natsci ~ scinec + as.factor(panelwave), index = c("idnum", "panelwave"), model = "fd", data = v)
summary(plm2)
clusterSE(fit = plm2, cluster.var = "idnum", data=v)
v$bsci = 4-v$scibnfts
v <- ddply(v, "idnum", mutate, d.bsci = firstD(bsci))
table(v$d.bsci)
prop.table(table(v$d.bsci))
plm3 <- plm(natsci ~ scinec + as.factor(panelwave) + as.factor(bsci), index = c("idnum", "panelwave"), model = "fd", data = v)
summary(plm3)
library(stargazer)
stargazer(lm2, plm2, plm3, title="Regression Results", align=TRUE, dep.var.labels=c("Federal Spending on Scientific Research"), covariate.labels=c("Pro Sci","2008","2010","Ben=Harm","Ben>Harm"), no.space=TRUE, column.labels=c("OLS", "First Diff A", "First Diff B"), dep.var.caption="", model.numbers=FALSE, type = "text", omit = "Constant")
install.packages("streamR")
library(streamR)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "gYIxV6TdEf9glxYWlGSGdBAiF"
consumerSecret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,
requestURL = requestURL, accessURL = accessURL, authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
library(RCurl)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "gYIxV6TdEf9glxYWlGSGdBAiF"
consumerSecret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,
requestURL = requestURL, accessURL = accessURL, authURL = authURL)
load("my_oauth")
library(ROAuth)
install.packages("ROAuth")
library(ROAuth)
library(RCurl)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- "gYIxV6TdEf9glxYWlGSGdBAiF"
consumerSecret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,
requestURL = requestURL, accessURL = accessURL, authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
save(my_oauth, file = "my_oauth.Rdata")
load("my_oauth.Rdata")
library(streamR)
filterStream("tweets.json", track = c("women", "men", timeout = 10, oauth = my_oauth))
filterStream("tweets.json", track = c("women", "men"), timeout = 10, oauth = my_oauth)
tweets.df <- parseTweets("tweets.json", simplify = TRUE)
c( length(grep("women", tweets.df$text, ignore.case = TRUE)),
length(grep("men", tweets.df$text, ignore.case = TRUE)) )
View(tweets.df)
table(tweets.df)
View(tweets.df)
list(tweets.df)
filterStream("tweetsUS.json", locations = c(-125, 25, -66, 50), timeout = 10,
oauth = my_oauth)
tweets.df <- parseTweets("tweetsUS.json", verbose = FALSE)
library(ggplot2)
library(grid)
map.data <- map_data("state")
install.packages("maps")
library(maps)
map.data <- map_data("state")
points <- data.frame(x = as.numeric(tweets.df$lon), y = as.numeric(tweets.df$lat))
points <- points[points$y > 25, ]
ggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = "white",
color = "grey20", size = 0.25) + expand_limits(x = map.data$long, y = map.data$lat) +
theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(),
axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(),
panel.grid.major = element_blank(), plot.background = element_blank(),
plot.margin = unit(0 * c(-1.5, -1.5, -1.5, -1.5), "lines")) + geom_point(data = points,
aes(x = x, y = y), size = 1, alpha = 1/5, color = "darkblue")
table(gss$race)
prop.table(table(gss$race))
find.package("devtools")
library(devtools)
find_rtools()
find.package("KernSmooth")
library(KernSmooth)
getwd()
setwd("~/GitHub/NYTtextanalysis/data/random2015dates")
options(stringsAsFactors = FALSE)
library(tm)
require(quanteda)
str(inaugTexts)
nyt.q <- read.csv("nyt.merged.csv")
textfile(nyt.q)
mytf7 <- textfile("nyt.merged.csv", textField = "text")
mytf7 <- textfile("nyt.merged.csv", textField = "TEXT")
summary(corpus(mytf7), 5)
sentences <- tokenize(mytf7[1:3], what = "sentence")
class(mytf7)
mytf6 <- as.character(mytf7)
S3Class(mytf7)
mytf7 <- textfile("nyt.merged.csv", textField = "TEXT", cache = FALSE,
docvarsfrom = c("filenames"), dvsep = "_", docvarnames = NULL)
class(inaugTexts)
nyt.merged <- read.csv("nyt.merged.csv")
nytcorpus <- VCorpus(DataframeSource(nyt.merged))
class(nytcorpus) #just making sure it's a corpus
f <- as.character(nytcorpus)
str(f)
myCorpus <- corpus(f)  # build the corpus
summary(f, n = 5)
sentences <- tokenize(f, what = "sentence")
sentences.n <- as.data.frame(sentences)
View(sentences.n)
sentences.n <- as.data.frame(unlist(sentences))
onlytext <- paste(nyt.merged$TEXT, collapse = " ", stringsAsFactors = FALSE)
class(onlytext)
f2 <- as.character(onlytext)
str(f2)
myCorpus2 <- corpus(f2)  # build the corpus
sentences2 <- tokenize(f2, what = "sentence")
sentences.n2 <- as.data.frame(unlist(sentences2))
View(sentences.n2)
install.packages("twitteR")
library(twitteR)
users <- c("krestenb", "tjb25587", "navidhassanpour", "schinria", "mandzz8")    # tjb25587 (me) has no followers
user <- getUser(x)
following <- user$getFriends()
sapply(users, function(x) {
user <- getUser(x)
following <- user$getFriends()
if (length(following) > 0) {        # ignore users with no followers
b <- twListToDF(following)
#f_count <- as.data.frame(b$followersCount)
u_id <- as.data.frame(b$id)
u_sname <- as.data.frame(b$screenName)
u_name <- as.data.frame(b$name)
#final_df <- cbind(u_id,u_name,u_sname,f_count)
final_df <- cbind(u_id,u_name,u_sname)
#sort_fc <- final_df[order(-f_count),]
#colnames(sort_fc) <- c('id','name','s_name','fol_count')
colnames(final_df) <- c('id','name','s_name','fol_count')
df_result <<- rbind(df_result, final_df)
}
})
library(streamR)
load("~/my_oauth.Rdata")
sapply(users, function(x) {
user <- getUser(x)
following <- user$getFriends()
if (length(following) > 0) {        # ignore users with no followers
b <- twListToDF(following)
#f_count <- as.data.frame(b$followersCount)
u_id <- as.data.frame(b$id)
u_sname <- as.data.frame(b$screenName)
u_name <- as.data.frame(b$name)
#final_df <- cbind(u_id,u_name,u_sname,f_count)
final_df <- cbind(u_id,u_name,u_sname)
#sort_fc <- final_df[order(-f_count),]
#colnames(sort_fc) <- c('id','name','s_name','fol_count')
colnames(final_df) <- c('id','name','s_name','fol_count')
df_result <<- rbind(df_result, final_df)
}
})
setwd("~/Columbia_QMSS/Spring_2016_classes/G4063_Data_Visualization")
load("./my_oauth.Rdata")
sapply(users, function(x) {
user <- getUser(x)
following <- user$getFriends()
if (length(following) > 0) {        # ignore users with no followers
b <- twListToDF(following)
#f_count <- as.data.frame(b$followersCount)
u_id <- as.data.frame(b$id)
u_sname <- as.data.frame(b$screenName)
u_name <- as.data.frame(b$name)
#final_df <- cbind(u_id,u_name,u_sname,f_count)
final_df <- cbind(u_id,u_name,u_sname)
#sort_fc <- final_df[order(-f_count),]
#colnames(sort_fc) <- c('id','name','s_name','fol_count')
colnames(final_df) <- c('id','name','s_name','fol_count')
df_result <<- rbind(df_result, final_df)
}
})
consumer.key <- 'gYIxV6TdEf9glxYWlGSGdBAiF'
consumer.secret <- "QIRynpLw0P6elikgsRyQeM7SzencY0Q1oGDwfb4zvzWpGBUe1K"
access.token <- "2851409691-N0t2hUTtkBkE3I8ZLdhkil9ThMaHRdIIPNJR0qk"
access.token.secret <- "5Mmv1zfyzceK8uWzsMZjd7iacue93skuL5pykOp48ib1F"
setup_twitter_oauth(consumer_key=consumer.key, consumer_secret=consumer.secret, access_token=access.token, access_secret=access.token.secret)
sapply(users, function(x) {
user <- getUser(x)
following <- user$getFriends()
if (length(following) > 0) {        # ignore users with no followers
b <- twListToDF(following)
#f_count <- as.data.frame(b$followersCount)
u_id <- as.data.frame(b$id)
u_sname <- as.data.frame(b$screenName)
u_name <- as.data.frame(b$name)
#final_df <- cbind(u_id,u_name,u_sname,f_count)
final_df <- cbind(u_id,u_name,u_sname)
#sort_fc <- final_df[order(-f_count),]
#colnames(sort_fc) <- c('id','name','s_name','fol_count')
colnames(final_df) <- c('id','name','s_name','fol_count')
df_result <<- rbind(df_result, final_df)
}
})
sapply(users, function(x) {
user <- getUser(x)
following <- user$getFriends()
if (length(following) > 0) {        # ignore users with no followers
b <- twListToDF(following)
#f_count <- as.data.frame(b$followersCount)
u_id <- as.data.frame(b$id)
u_sname <- as.data.frame(b$screenName)
u_name <- as.data.frame(b$name)
#final_df <- cbind(u_id,u_name,u_sname,f_count)
final_df <- cbind(u_id,u_name,u_sname)
#sort_fc <- final_df[order(-f_count),]
#colnames(sort_fc) <- c('id','name','s_name','fol_count')
colnames(final_df) <- c('id','name','s_name')
df_result <<- rbind(df_result, final_df)
}
})
df_result <- data.frame(t(rep(NA, 3)))
names(df_result) <- c('id', 'name', 's_name')
df_result <- df_result[0:0,]
sapply(users, function(x) {
user <- getUser(x)
following <- user$getFriends()
if (length(following) > 0) {        # ignore users with no followers
b <- twListToDF(following)
#f_count <- as.data.frame(b$followersCount)
u_id <- as.data.frame(b$id)
u_sname <- as.data.frame(b$screenName)
u_name <- as.data.frame(b$name)
#final_df <- cbind(u_id,u_name,u_sname,f_count)
final_df <- cbind(u_id,u_name,u_sname)
#sort_fc <- final_df[order(-f_count),]
#colnames(sort_fc) <- c('id','name','s_name','fol_count')
colnames(final_df) <- c('id','name','s_name')
df_result <<- rbind(df_result, final_df)
}
})
install.packages("smappR")
seed <- getUser(c("navidhassanpour", "mandzz8", "schinria"))
seed <- getUser("mandzz8")
seed2 <- getUser("schinria")
users <- seed + seed2
users <- rbind(seed, seed2)
seed <- getUser("navidhassanpour", "mandzz8", "schinria")
friends <- seed$getFriends(seed)
friends <- seed$getFriends()
summary(friends)
friends2 <- users$getFriends
install.packages("rvest")
library(rvest)
doc <- read_html("ftp://cran.r-project.org/pub/R/web/packages/rvest/vignettes/selectorgadget.html")
date <- doc %>%  #pipeline operator - takes apart nested structure to chain structure
html_nodes('.date em') %>%
html_text()
date
lego_movie <- read_html("http://www.imdb.com/title/tt1490017/")
rating <- lego_movie %>%
html_nodes('strong span') %>%
html_text()
rating # prints rating we selected
title <- lego_movie %>%
html_nodes('h1') %>%
html_text()
title
year <- lego_movie %>%
html_nodes('#titleYear a') %>%
html_text() %>%
as.numeric()
year
cast <- lego_movie %>%
html_nodes('.itemprop') %>%
html_text() %>%
html_text()
cast <- lego_movie %>%
html_nodes('.itemprop') %>%
html_text()
cast
cast <- lego_movie %>%
html_nodes('.itemprop .itemprop') %>%
html_text()
cast
tail(cast, 10)
lego_movie_full_cast <- read_html("http://www.imdb.com/title/tt1490017/fullcredits?ref_=tt_cl_sm#cast")
cast <- lego_movie_full_cast %>%
html_nodes('.itemprop .itemprop') %>%
html_text()
cast
img <- lego_movie %>%
html_node('#title-overview-widget img') %>%
html_attr('src')
img
install.packages('httr')
install.packages("httr")
library(httr)
library("httr", lib.loc="~/R/win-library/3.2")
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=columbia%2Buniversity&begin_date=20150101&end_date=20151231&sort=oldest&api-key=f10de54cc5b7b095128cb4e4d904fca5%3A18%3A74818718'
response <- httr::GET(url)
response
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
library(httr)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=columbia%2Buniversity&begin_date=20150101&end_date=20151231&sort=oldest&api-key=f10de54cc5b7b095128cb4e4d904fca5%3A18%3A74818718'
response <- httr::GET(url) #url is response object
response # should show date, status = 200, json content type, size
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
results <- content(response, as = 'parsed') #this is a json file format, then turn format into a list
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=columbia%2Buniversity&begin_date=20150101&end_date=20151231&sort=oldest&api-key=f10de54cc5b7b095128cb4e4d904fca5%3A18%3A74818718"
response <- httr::GET(url) #url is response object
response # should show date, status = 200, json content type, size
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
str(results)
results <- content(response) #this is a json file format, then turn format into a list
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
results <- content(response, parsed) #this is a json file format, then turn format into a list
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
library(httr)
install.packages('httr')
# read response content
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=columbia%2Buniversity&begin_date=20150101&end_date=20151231&sort=oldest&api-key=f10de54cc5b7b095128cb4e4d904fca5%3A18%3A74818718"
library(httr)
response # should show date, status = 200, json content type, size
str(results)
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
response <- httr::GET(url) #url is response object
names(results)
install.packages("httr")
install.packages("httr")
install.packages("httr")
library(httr)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=columbia%2Buniversity&begin_date=20150101&end_date=20151231&sort=oldest&api-key=f10de54cc5b7b095128cb4e4d904fca5%3A18%3A74818718"
response <- httr::GET(url) #url is response object
response # should show date, status = 200, json content type, size
# read response content
results <- content(response, 'parsed') #this is a json file format, then turn format into a list
str(results)
names(results)
names(results$response)
length(results$response$docs)
results$response$docs[1]
str(f)
str(f2)
summary(f2, n = 5)
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) #creates function "toSpace" using gsub that replaces characters with a space
getwd()
setwd("~/GitHub/NYTtextanalysis/data/random2015dates")
options(stringsAsFactors = FALSE)
library(tm)
require(quanteda)
onlytext <- paste(nyt.merged$TEXT, collapse = " ", stringsAsFactors = FALSE)
class(onlytext)
onlytextvs <- VectorSource(onlytext)
onlytextcorpus <- Corpus(onlytextvs)
sentences3 <- tokenize(onlytext, what = "sentence")
sentences.n3 <- as.data.frame(unlist(sentences3))
View(sentences.n3)
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) #creates function "toSpace" using gsub that replaces characters with a space
otc <- tm_map(onlytextcorpus, content_transformer(tolower)) #need to use content_transformer with tolower because of bug in newer version of tm package
otc <- tm_map(otc, stripWhitespace)
class(otc)
f3 <- as.character(otc) #uses only TEXT corpus (created from TEXT column) that has been changed to all lowercase & stripWhitespace
str(f3)
myCorpus3 <- corpus(f3)  # build the corpus
summary(f3)
sentences3 <- tokenize(f3, what = "sentence")
sentences.n3 <- as.data.frame(unlist(sentences3))
class(onlytext)
sentences2 <- tokenize(onlytext, what = "sentence")
sentences.n2 <- as.data.frame(unlist(sentences2))
head(NAMES, 20)
sentences.n2.lower <- toLower(sentences.n2, keepAcronyms = FALSE)
sentences2.lower <- toLower(sentences2, keepAcronyms = FALSE)
sentences.n2.lower <- as.data.frame(unlist(sentences2.lower))
View(sentences.n2.lower)
kwic(sentences2.lower, "terror", valuetype = "regex")
otc2 <- tm_map(otc, removePunctuation)
otc2 <- tm_map(otc2, toSpace, "-")
otc2 <- tm_map(otc2, toSpace, ":")
otc2 <- tm_map(otc2, removeWords, stopwords("english"))
otc2 <- tm_map(otc2, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
otcstem <- tm_map(otc2, stemDocument) #uses tm package stemming
dtmstem <- DocumentTermMatrix(otcstem) # for corpus w/o stopwords and w/ tm stemming
dtmstem2 <- as.matrix(dtmstem)
dtmstem2.a <- sort(rowSums(dtmstem2),decreasing=TRUE)
dtmstem2.d <- data.frame(word = names(dtmstem2.a),freq=dtmstem2.a)
head(dtmstem2.d, 10)
dtmstem2.d <- data.frame(word = names(dtmstem2),freq=dtmstem2.a)
dtmstem2.d <- data.frame(word = names(dtmstem2.a),freq=dtmstem2.a)
findFreqTerms(dtmstem, lowfreq = 100)
findFreqTerms(dtmstem, lowfreq = 600)
barplot(dtmstem2.d[1:15,]$freq, las = 2, names.arg = dtmstem2.d[1:15,]$word,
col ="lightblue", main ="Most Frequently-Used Words in Sample",
ylab = "Word Frequencies")
library(quanteda)
NCgenderDict <- dictionary(list(
man = c('guy','spokesman','chairman',"men's",'men','him',"he's",'his','boy',
'boyfriend','boyfriends','boys','brother','brothers','dad','dads','dude',
'father','fathers','fiance','gentleman','gentlemen','god','grandfather',
'grandpa','grandson','groom','he','himself','husband','husbands','king',
'male','man','mr','nephew','nephews','priest','prince','son','sons',
'uncle','uncles','waiter','widower','widowers'),
woman = c('heroine','spokeswoman','chairwoman',"women's",'actress','women',
"she's",'her','aunt','aunts','bride','daughter','daughters','female',
'fiancee','girl','girlfriend','girlfriends','girls','goddess',
'granddaughter','grandma','grandmother','herself','ladies','lady',
'lady','mom','moms','mother','mothers','mrs','ms','niece','nieces',
'priestess','princess','queens','she','sister','sisters','waitress',
'widow','widows','wife','wives','woman')))
nyt.dfm <- dfm(sentences2.lower, dictionary = NCgenderDict)
nyt.dfm$man
head(nyt.dfm, 30)
nyt.dfm
nyt.dfm <- dfm(onlytext, dictionary = NCgenderDict)
nyt.dfm
class(sentences2.lower)
sentences2.lower.corpus <- VCorpus(sentences2.lower)
class(onlytext)
sentences2.lower.vs <- VectorSource(sentences2.lower)
sentences2.lower.corpus <- Corpus(sentences2.lower.vs)
nyt.dfm2 <- dfm(sentences2.lower.corpus, dictionary = NCgenderDict)
class(onlytext)
g <- as.character(sentences2.lower)
nyt.dfm2 <- dfm(g, dictionary = NCgenderDict)
nyt.dfm
nyt.dfm2
g.df <- as.data.frame(unlist(nyt.dfm2))
View(g.df)
pmatch(NCgenderDict, g, nnomatch = 0)
pmatch(NCgenderDict, g, nomatch = 0)
class(NCgenderDict)
NCgenderDict$man
pmatch(NCgenderDict$man, g, nomatch = 0)
