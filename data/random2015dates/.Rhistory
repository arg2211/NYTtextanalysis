nytcorpus <- VCorpus(DataframeSource(nyt.merged))
class(nytcorpus) #just making sure it's a corpus
inspect(nytcorpus) #gives info about all "documents" in corpus
onlytext <- paste(nyt.merged$TEXT, collapse = " ", stringsAsFactors = FALSE)
onlytextvs <- VectorSource(onlytext)
onlytextcorpus <- Corpus(onlytextvs)
class(onlytextcorpus)
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) #creates function "toSpace" using gsub that replaces characters with a space
otc <- tm_map(onlytextcorpus, content_transformer(tolower)) #need to use content_transformer with tolower because of bug in newer version of tm package
otc <- tm_map(otc, stripWhitespace)
otc2 <- tm_map(otc, removePunctuation)
otc2 <- tm_map(otc2, toSpace, "-")
otc2 <- tm_map(otc2, toSpace, ":")
otc2 <- tm_map(otc2, removeWords, stopwords("english"))
otc2 <- tm_map(otc2, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
otcstem <- tm_map(otc2, stemDocument) #uses tm package stemming
dtm <- DocumentTermMatrix(otc) # for original corpus w/o stopwords
dtm2 <- as.matrix(dtm)
freq <- colSums(dtm2)
count <- rowSums(dtm2)
freq <- sort(freq, decreasing = TRUE)
require(quanteda)
sentences3 <- tokenize(onlytext, what = "sentence")
sentences.n3 <- as.data.frame(unlist(sentences3)) #create data frame of split sentences
sentences3.lower <- toLower(sentences3, keepAcronyms = FALSE) #make all sentences lowercase
sentences.n3.lower <- as.data.frame(unlist(sentences3.lower)) # create dataframe of lowercase sentences
colnames(sentences.n3.lower) = c("all")
sentences.n3.lower$men <- ifelse(grepl("\\b(guys?|spokesm[ae]n|chairm[ae]n|m[ae]n
|him|he|his|boys?|boyfriends?|brothers?|dads?|dudes?|fathers?|gentlem[ae]n|gods?
|grandfathers?|grandpas?|grandsons?|grooms?|himself|hisself|husbands?|kings?|males?
|mr|nephews?|priests?|princes?|sons?|uncles?|widowers?)\\b",
sentences.n3.lower$all, ignore.case = TRUE), 1, 0)
sentences.n3.lower$women <- ifelse(grepl("\\b(heroines?|spokeswom[ae]n|chairwom[ae]n
|wom[ae]n|actress|actresses|she|her|aunts?|brides?|daughters?|females?|girls?|girlfriends?
|goddess|goddesses|granddaughters?|grandmas?|grandmothers?|herself|ladies|lady|moms?
|mothers?|mrs|ms|nieces?|priestess|priestesses|princess|princesses|queens?|sisters?
|waitress|waitresses|widows?|wife|wives)\\b",
sentences.n3.lower$all, ignore.case = TRUE), 1, 0)
sentences.n3.lower$both <- ifelse((sentences.n3.lower$men==1 & sentences.n3.lower$women==1), 1, 0)
sentences.n3.lower$none <- ifelse((sentences.n3.lower$men==0 & sentences.n3.lower$women==0), 1, 0)
sum(sentences.n3.lower$none) # 56,237 sentences that are about men, women, or both
sum(sentences.n3.lower$both) # 2,544 sentences that contain words about both men and women
s.men <- subset(sentences.n3.lower, men==1 & women!=1)
s.women <- subset(sentences.n3.lower, women==1 & men!=1)
library(tm)
corpus.men <- VCorpus(DataframeSource(s.men))
corpus.men.text <- paste(s.men$all, collapse = " ", stringsAsFactors = FALSE)
corpus.men.textVS <- VectorSource(corpus.men.text)
c.men <- Corpus(corpus.men.textVS)
class(c.men)
m <- tm_map(c.men, stripWhitespace)
m <- tm_map(m, removePunctuation)
m <- tm_map(m, toSpace, "-")
m <- tm_map(m, toSpace, ":")
m <- tm_map(m, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
dtm.men <- DocumentTermMatrix(m) # for original corpus w/o stopwords
dtm.men2 <- as.matrix(dtm.men)
dtm.men # gives number of terms in documents
# find most frequent terms
freq.men <- colSums(dtm.men2)
count.men <- rowSums(dtm.men2)
count.men
freq.men <- sort(freq.men, decreasing = TRUE)
head(freq.men, 30)
m2 <- tm_map(m, removeWords, stopwords("english"))
m2.stem <- tm_map(m2, stemDocument) #uses tm package stemming
dtm.men.x <- as.matrix(dtm.men)
dtm.men2 <- DocumentTermMatrix(m2) # for original corpus w/o stopwords
dtm.men2.x <- as.matrix(dtm.men.x)
dtm.men <- DocumentTermMatrix(m) # for original corpus w/o stopwords
dtm.men.x <- as.matrix(dtm.men)
dtm.men2 <- DocumentTermMatrix(m2) # for original corpus w/o stopwords
dtm.men2.x <- as.matrix(dtm.men2)
dtm.men2.stem <- DocumentTermMatrix(m2.stem) # for original corpus w/o stopwords
dtm.men2.stem.x <- as.matrix(dtm.men2.stem)
freq.men <- colSums(dtm.men.x)
count.men <- rowSums(dtm.men.x)
freq.men <- sort(freq.men, decreasing = TRUE)
words.men <- names(freq.men)
wordcloud(words.men[1:100], freq.men[1:100])
library(wordcloud)
library(RColorBrewer)
wordcloud(words.men[1:100], freq.men[1:100])
wordcloud(names(freq.men), freq.men, max.words=100) #creates word cloud of words, by frequency (larger text = more), with max of 100 words displayed
wordcloud(names(freq.men), freq.men, min.freq=600, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
freq.men2 <- colSums(dtm.men2.x)
count.men2 <- rowSums(dtm.men2.x)
count.men2
freq.men2 <- sort(freq.men2, decreasing = TRUE)
head(freq.men2, 30)
freq.men2.stem <- colSums(dtm.men2.stem.x)
count.men2.stem <- rowSums(dtm.men2.stem.x)
freq.men2.stem <- sort(freq.men2.stem, decreasing = TRUE)
head(freq.men2.stem, 30)
words.men2 <- names(freq.men2)
wordcloud(words.men2[1:100], freq.men2[1:100])
wordcloud(names(freq.men2), freq.men2, max.words=100) #creates word cloud of words, by frequency (larger text = more), with max of 100 words displayed
wordcloud(names(freq.men2), freq.men2, min.freq=600, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq.men2), freq.men2, min.freq=300, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq.men2), freq.men2, min.freq=200, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
words.men2.stem <- names(freq.men2.stem)
wordcloud(words.men2.stem[1:100], freq.men2.stem[1:100])
wordcloud(names(freq.men2.stem), freq.men2.stem, max.words=100) #creates word cloud of words, by frequency (larger text = more), with max of 100 words displayed
wordcloud(names(freq.men2.stem), freq.men2.stem, min.freq=200, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq.men2.stem), freq.men2.stem, min.freq=250, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
count.men2.stem
head(freq.men2.stem, 30)
corpus.women <- VCorpus(DataframeSource(s.women))
inspect(corpus.women[1])
corpus.women.text <- paste(s.women$all, collapse = " ", stringsAsFactors = FALSE)
corpus.women.textVS <- VectorSource(corpus.women.text)
c.women <- Corpus(corpus.women.textVS)
class(c.women)
w <- tm_map(c.women, stripWhitespace)
w <- tm_map(w, removePunctuation)
w <- tm_map(w, toSpace, "-")
w <- tm_map(w, toSpace, ":")
w <- tm_map(w, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
w2 <- tm_map(w, removeWords, stopwords("english"))
w2.stem <- tm_map(w2, stemDocument) #uses tm package stemming
dtm.women <- DocumentTermMatrix(w) # for original corpus w/o stopwords
dtm.women.x <- as.matrix(dtm.women)
dtm.women # gives number of terms in documents
dtm.women2 <- DocumentTermMatrix(w2) # for original corpus w/o stopwords
dtm.women2.x <- as.matrix(dtm.women2)
dtm.women2 # gives number of terms in documents
dtm.women2.stem <- DocumentTermMatrix(w2.stem) # for original corpus w/o stopwords
dtm.women2.stem.x <- as.matrix(dtm.women2.stem)
dtm.women2.stem # gives number of terms in documents
dtm.men2.stem # gives number of terms in documents
freq.women <- colSums(dtm.women.x)
count.women <- rowSums(dtm.women.x)
count.women
freq.women <- sort(freq.women, decreasing = TRUE)
head(freq.women, 30)
freq.women2 <- colSums(dtm.women2.x)
count.women2 <- rowSums(dtm.women2.x)
count.women2
freq.women2 <- sort(freq.women2, decreasing = TRUE)
head(freq.women2, 30)
freq.women2.stem <- colSums(dtm.women2.stem.x)
count.women2.stem <- rowSums(dtm.women2.stem.x)
count.women2.stem
freq.women2.stem <- sort(freq.women2.stem, decreasing = TRUE)
head(freq.women2.stem, 30)
setwd("~/Columbia_QMSS/Spring_2016_classes/G4062_Social_Network_Analysis/Lab 2")
library(igraph)
setwd("~/Columbia_QMSS/Spring_2016_classes/G4062_Social_Network_Analysis/Lab 3")
vdb.matrix.t1 <- as.matrix(read.csv('./vdb-time-1.csv',header=FALSE,row.names=NULL,check.names=FALSE))
vdb.matrix.t6 <- as.matrix(read.csv('./vdb-time-6.csv',header=FALSE,row.names=NULL,check.names=FALSE))
vdb.attr <- read.csv('./vdb-attr.csv', header=TRUE)
t1 <- as.matrix(read.csv('./vdb-time-1.csv',header=FALSE,row.names=NULL,check.names=FALSE))
t6 <- as.matrix(read.csv('./vdb-time-6.csv',header=FALSE,row.names=NULL,check.names=FALSE))
a <- read.csv('./vdb-attr.csv', header=TRUE)
t1.g=graph.adjacency(t1,mode="min",weighted=NULL) ## min means that only recipricated ties are counted ##
t6.g=graph.adjacency(t6,mode="min",weighted=NULL)
graph.density(t1.g) ## get a sense of overal graph density ##
graph.density(t6.g)
t1.gervin = edge.betweenness.community (t1.g, directed = TRUE, edge.betweenness = TRUE, merges = TRUE,
bridges = TRUE, modularity = TRUE, membership = TRUE)
plot(t1.gervin, t1.g)  ## plot G-N partitioning
t6.gervin = edge.betweenness.community (t6.g, directed = TRUE, edge.betweenness = TRUE, merges = TRUE,
bridges = TRUE, modularity = TRUE, membership = TRUE)
plot(t6.gervin, t6.g)
t1.walk <- walktrap.community(t1.g, steps=200,modularity=TRUE)  ## run random walk partitioning
t6.walk <- walktrap.community(t6.g, steps=200,modularity=TRUE)
plot(t1.walk, t1.g)  ## plot R-W partitioning
plot(t6.walk, t6.g)
t1.labelp = label.propagation.community(t1.g)  ## run label propogation partitioning
plot(t1.labelp, t1.g)  ## plot L-P partitioning
t6.labelp = label.propagation.community(t6.g)
plot(t6.labelp, t6.g)
compare(t1.gervin, t1.walk, method= c("nmi")) # 0.91 pretty higher
compare(t1.gervin, t1.walk, method= c("rand"))
compare(t1.gervin, t1.walk, method= c("adjusted.rand")) #
compare(t6.gervin, t6.walk, method= c("nmi")) # 0.91 pretty higher
compare(t6.gervin, t6.walk, method= c("rand"))
compare(t6.gervin, t6.walk, method= c("adjusted.rand")) #
compare(t1.gervin, t1.labelp, method= c("nmi"))
compare(t1.gervin, t1.labelp, method= c("rand"))
compare(t1.gervin, t1.labelp, method= c("adjusted.rand")) #
compare(t1.gervin, t1.walk, method= c("nmi")) # 0.37 pretty low
compare(t1.gervin, t1.walk, method= c("rand")) # 0.42
compare(t1.gervin, t1.walk, method= c("adjusted.rand")) # 0.13
compare(t6.gervin, t6.labelp, method= c("nmi"))
compare(t6.gervin, t6.labelp, method= c("rand"))
compare(t6.gervin, t6.labelp, method= c("adjusted.rand"))
girvan = data.frame(t1.g$membership)
walk = data.frame(t1.walk$membership)
labelp = data.frame(t1.labelp$membership)
t1.df=data.frame(name=V(t1.g)$name)
t1.new <- cbind(a, girvan, walk, labelp)
View(a)
girvan = data.frame(t1.g$membership)
girvan = data.frame(t1.gervin$membership)
t1.new <- cbind(a, girvan, walk, labelp)
View(t1.new)
gn.groups <- community.to.membership(t1.g, t1.gervin$merge, steps = ideal_steps)$membership
gn.groups <- cutat(t1.g, t1.gervin$merges, steps = ideal_steps)$membership
gn.groups <- community.to.membership(t1.g, t1.gervin$merges, steps = ideal_steps)$membership
gn.groups <- cutat(t1.gervin, t1.gervin$merges, steps = ideal_steps)$membership
str(w2.stem)
w2.stem <- tm_map(w2, stemDocument) #uses tm package stemming
w2.stem <- tm_map(w2.stem, PlainTextDocument)
str(w2.stem)
str(m2.stem)
setwd("~/GitHub/NYTtextanalysis/data/random2015dates")
write(w2.stem, file = "sent_women.txt")
data("crude")
writeCorpus(crude, path = ".",filenames = paste(seq_along(crude), ".txt", sep = ""))
str(crude)
class(crude)
writeCorpus(w2.stem, path = "./sentences", filenames = "women.txt", sep = "")
writeCorpus(w2.stem, path = "./sentences", filenames = "women.txt")
writeCorpus(m2.stem, path = "./sentences", filenames = "men.txt")
class(c.men)
writeCorpus(c.women, path = "./sentencesOG", filenames = "OGwomen.txt")
writeCorpus(c.men, path = "./sentencesOG", filenames = "OGmen.txt")
class(s.men)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
library(tm)
library(SnowballC)
library(Rcampdf)
fp <- file.path("~/Documents/GitHub/NYTtextanalysis/data/random2015dates", "sentencesOG")
dir(fp)
fp <- file.path("~/Documents/GitHub/NYTtextanalysis/data/random2015dates/sentencesOG", "texts")
dir(fp)
fp <- file.path("Users/Amanda/Documents/GitHub/NYTtextanalysis/data/random2015dates/sentencesOG", "texts")
dir(fp)
fp <- file.path("Users/Amanda/Documents/GitHub/NYTtextanalysis/data/random2015dates", "sentencesOG")
dir(fp)
fp
dir(fp)
docs <- Corpus(DirSource(fp))
fp <- file.path("Users/Amanda/Documents/GitHub/NYTtextanalysis/data/random2015dates", "sentences")
dir(fp)
docs <- Corpus(DirSource(fp))
getwd()
fp <- file.path(".", "sentences")
dir(fp)
docs <- Corpus(DirSource(fp))
summary(docs)
inspect(docs[1])
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument) #uses tm package stemming
docs <- tm_map(docs, stripWhitespace)
class(docs)
docs <- tm_map(docs, PlainTextDocument)
class(docs)
docs <- Corpus(VectorSource(docs))
docs.dtm <- DocumentTermMatrix(docs)
docs.dtm
docs.dtm.matrix = as.matrix(dtm)
docs.dtm.df = as.data.frame(m.dtm)
docs.dtm.matrix = as.matrix(docs.dtm)
docs.dtm.df = as.data.frame(docs.dtm.matrix)
View(docs.dtm.df)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, PlainTextDocument)
docs <- Corpus(VectorSource(docs))
docs.dtm <- DocumentTermMatrix(docs)
docs.dtm # 34% sparsity, 2 documents, 29,578 terms
docs.dtm.matrix = as.matrix(docs.dtm)
docs.dtm.df = as.data.frame(docs.dtm.matrix)
docs <- tm_map(docs, toSpace, "창")
docs <- tm_map(docs, toSpace, "찾")
docs <- Corpus(VectorSource(docs))
docs.dtm <- DocumentTermMatrix(docs)
docs.dtm # 34% sparsity, 2 documents, 28,249 terms
docs.dtm.matrix = as.matrix(docs.dtm)
docs.dtm.df = as.data.frame(docs.dtm.matrix)
docs <- tm_map(docs, toSpace, "%")
docs <- Corpus(VectorSource(docs))
docs.dtm <- DocumentTermMatrix(docs)
docs.dtm # 34% sparsity, 2 documents, 28,127 terms
docs.dtm.matrix = as.matrix(docs.dtm)
docs.dtm.df = as.data.frame(docs.dtm.matrix)
View(docs.dtm.df)
findFreqTerms(docs.dtm, lowfreq=20)
findFreqTerms(docs.dtm, lowfreq=500)
findFreqTerms(docs.dtm[,1], lowfreq=500) # lowest freq = 500 times
findFreqTerms(docs.dtm[1,], lowfreq=500) # lowest freq = 500 times
findFreqTerms(docs.dtm[2,], lowfreq=500) # lowest freq = 500 times
docs.dtm[1,]
length(docs.dtm[1,])
docs.dtm.t <- t(docs.dtm)
head(docs.dtm.t)
docs.dtm.df.t <- t(docs.dtm.df)
View(docs.dtm.df.t)
dtm.men2.stem # gives number of terms in documents
dtm.women2.stem # gives number of terms in documents
head(docs.dtm.t)
head(docs.dtm.df.t)
cor(docs.dtm.df.t)
install.packages("lsa")
library(lsa)
cosine(docs.dtm.df.t)
chitable <- table(docs.dtm.df.t)
chisq.test(chitable)
docs.dtm.s <- removeSparseTerms(dtm, 0.05) # This makes a matrix that is 5% empty space, maximum.
inspect(docs.dtm.s)
docs.dtm.s
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.05) # This makes a matrix that is 5% empty space, maximum.
docs.dtm.s
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.10) # This makes a matrix that is 5% empty space, maximum.
docs.dtm.s
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.20) # This makes a matrix that is 5% empty space, maximum.
docs.dtm.s
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.30) # This makes a matrix that is 20% empty space, maximum.
docs.dtm.s
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.40) # This makes a matrix that is 20% empty space, maximum.
docs.dtm.s
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.50) # This makes a matrix that is 20% empty space, maximum.
docs.dtm.s
docs.dtm # 34% sparsity, 2 documents, 28,127 terms
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.20) # This makes a matrix that is 20% empty space, maximum.
docs.dtm.s
terms <-DocumentTermMatrix(docs,control=list(weighting=function(x) weightTfIdf(x,normalize=FALSE)))
terms
head(terms)
terms.matrix = as.matrix(terms)
terms.df = as.data.frame(terms.matrix)
View(terms.matrix)
terms.df.t <- t(terms.df)
View(terms.df.t)
findFreqTerms(terms[1,], lowfreq=4)
findFreqTerms(terms[1,], lowfreq=10)
findFreqTerms(terms[1,], lowfreq=50)
findFreqTerms(terms[1,], lowfreq=40)
findFreqTerms(terms[2,], lowfreq=40)
summing = function(x) x/sum(x, na.rm=T)
docs.dtm.df.t_new <- apply(docs.dtm.df.t, 2, summing)
head(docs.dtm.df.t_new)
View(docs.dtm.df.t_new)
docs.dtm.df.t_new_df = as.data.frame(docs.dtm.df.t_new)
docs.dtm.df.t_new_ <- as.data.frame(docs.dtm.df.t_new)
docs.dtm.df.t_new <- as.data.frame(docs.dtm.df.t_new)
names(docs.dtm.df.t_new)[names(docs.dtm.df.t_new)=="1"] <- "men"
names(docs.dtm.df.t_new)[names(docs.dtm.df.t_new)=="2"] <- "women"
docs.dtm.df.t_new$ratio = docs.dtm.df.t_new$men - docs.dtm.df.t_new$women
sort.men <- docs.dtm.df.t_new[order(-docs.dtm.df.t_new$ratio) , ]
sort.men[1:15, ]
sort.women <- docs.dtm.df.t_new[order(docs.dtm.df.t_new$ratio) , ]
sort.women[1:15, ]
getReaders()
summary(nytcorpus)
inspect(nytcorpus[2]) #gives info about only the 2nd "document" in corpus
inspect(nytcorpus[20]) #gives info about only the 2nd "document" in corpus
class(onlytextcorpus)
getTransformations()
z <- data.frame(as.matrix(onlytextcorpus))
z <- data.frame(as.matrix(onlytext))
View(z)
otc <- tm_map(onlytextcorpus, content_transformer(tolower)) #need to use content_transformer with tolower because of bug in newer version of tm package
otc <- tm_map(otc, stripWhitespace)
otc2 <- tm_map(otc, removePunctuation)
otc2 <- tm_map(otc2, removeNumbers)
otc2 <- tm_map(otc2, toSpace, "-")
otc2 <- tm_map(otc2, toSpace, ":")
otc2 <- tm_map(otc2, toSpace, "%")
otc2 <- tm_map(otc2, removeWords, stopwords("english"))
otc2 <- tm_map(otc2, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
otc2 <- tm_map(otc2, stripWhitespace)
otcstem <- tm_map(otc2, stemDocument) #uses tm package stemming
dtm <- DocumentTermMatrix(otc)
nyt.merged <- read.csv("nyt.merged.csv")
library(tm)
onlytext <- paste(nyt.merged$TEXT, collapse = " ", stringsAsFactors = FALSE)
onlytextcorpus <- Corpus(VectorSource(onlytext))
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})
otc <- tm_map(onlytextcorpus, content_transformer(tolower)) #need to use content_transformer with tolower because of bug in newer version of tm package
otc <- tm_map(otc, stripWhitespace)
otc2 <- tm_map(otc, removePunctuation)
otc2 <- tm_map(otc2, removeNumbers)
otc2 <- tm_map(otc2, toSpace, "-")
otc2 <- tm_map(otc2, toSpace, ":")
otc2 <- tm_map(otc2, toSpace, "%")
otc2 <- tm_map(otc2, removeWords, stopwords("english"))
otc2 <- tm_map(otc2, removeWords, c("url")) #insert words that you want to remove from corpus where "x" is
otc2 <- tm_map(otc2, stripWhitespace)
otcstem <- tm_map(otc2, stemDocument) #uses tm package stemming
dtm.otc <- DocumentTermMatrix(otc)
dtm.m.otc <- as.matrix(dtm)
dtm.m.otc <- as.matrix(dtm.otc)
dtm.otc # gives number of terms in documents
dtm.otc2 <- DocumentTermMatrix(otc2)
dtm.m.otc2 <- as.matrix(dtm.otc2)
dtm.otc2
dtm.stem <- DocumentTermMatrix(otcstem)
dtm.m.otcstem <- as.matrix(dtm.stem)
dtm.stem
freq.otc <- colSums(dtm.otc)
freq.otc <- colSums(dtm.m.otc)
count.otc <- rowSums(dtm.m.otc)
freq.otc2 <- colSums(dtm.m.otc2)
count.otc2 <- rowSums(dtm.m.otc2)
freq.otc <- sort(freq.otc, decreasing = TRUE)
count.otc
View(dtm.m.otc)
freq.otc2 <- sort(freq.otc2, decreasing = TRUE)
count.otc2
head(table(freq.otc), 30) #shows you a table of frequencies (how many words [bottom row] appear this frequently [top row])
freq.otcstem <- colSums(dtm.m.otcstem)
freq.otcstem <- sort(freq.otcstem, decreasing = TRUE)
head(freq.otcstem, 30) # gives top 30 most used words
count.stem <- rowSums(dtm.m.otcstem)
count.stem
wordcloud(names(freq.otcstem), freq.otcstem, min.freq=600, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq.otcstem), freq.otcstem, min.freq=1000, colors=brewer.pal(8, "Dark2")) #creates word cloud of words, by frequency (larger text = more), with only words that occur 1000+ times
wordcloud(names(freq.otcstem), freq.otcstem, min.freq=1000, colors=brewer.pal(8, "Dark2"), random.order = FALSE)
wordcloud(names(freq), freq, max.words = 100)
wordcloud(names(freq.otcstem), freq.otcstem, max.words = 100)
require(quanteda)
class(inaugTexts) #inaugTexts is used in example in documentation
class(onlytext)
sentences <- tokenize(onlytext, what = "sentence")
sentences <- toLower(sentences, keepAcronyms = FALSE) # make all sentences lowercase
sentences.df <- as.data.frame(unlist(sentences)) # create dataframe of lowercase sentences
View(sentences.df)
colnames(sentences.df) = c("all")
sentences.df$men <- ifelse(grepl("\\b(guys?|spokesm[ae]n|chairm[ae]n|m[ae]n
|him|he|his|boys?|boyfriends?|brothers?|dads?|dudes?|fathers?|gentlem[ae]n|gods?
|grandfathers?|grandpas?|grandsons?|grooms?|himself|hisself|husbands?|kings?|males?
|mr|nephews?|priests?|princes?|sons?|uncles?|widowers?)\\b",
sentences.df$all, ignore.case = TRUE), 1, 0)
sentences.df$women <- ifelse(grepl("\\b(heroines?|spokeswom[ae]n|chairwom[ae]n
|wom[ae]n|actress|actresses|she|her|aunts?|brides?|daughters?|females?|girls?|girlfriends?
|goddess|goddesses|granddaughters?|grandmas?|grandmothers?|herself|ladies|lady|moms?
|mothers?|mrs|ms|nieces?|priestess|priestesses|princess|princesses|queens?|sisters?
|waitress|waitresses|widows?|wife|wives)\\b",
sentences.df$all, ignore.case = TRUE), 1, 0)
sentences.df$both <- ifelse((sentences.df$men==1 & sentences.df$women==1), 1, 0)
sentences.df$none <- ifelse((sentences.df$men==0 & sentences.df$women==0), 1, 0)
sum(sentences.df$none) # 56,237 sentences that are about men, women, or both
sum(sentences.df$both) # 2,544 sentences that contain words about both men and women
sum(sentences.df$men)
sum(sentences.df$women) #
s.men <- subset(sentences.df, men==1 & women!=1)
s.women <- subset(sentences.df, women==1 & men!=1)
sum(s.women$both) # check to make sure there are no words that fit the 'none' or 'both' category
sum(s.men$both)
sum(s.women$none)
sum(s.men$none)
corpus.men <- VCorpus(DataframeSource(s.men))
corpus.women <- VCorpus(DataframeSource(s.women))
class(corpus.men) #just making sure it's a corpus
inspect(corpus.men[100]) #gives info about only the 100th "document" in corpus
corpus.men <- paste(s.men$all, collapse = " ", stringsAsFactors = FALSE)
corpus.men <- Corpus(VectorSource(corpus.men))
corpus.women <- paste(s.women$all, collapse = " ", stringsAsFactors = FALSE)
corpus.women <- Corpus(VectorSource(corpus.women.text))
corpus.women <- Corpus(VectorSource(corpus.women))
writeCorpus(corpus.women, path = "./sentences", filenames = "women.txt")
writeCorpus(corpus.men, path = "./sentences", filenames = "men.txt")
library(tm)
library(SnowballC)
library(Rcampdf)
fp <- file.path(".", "sentences")
dir(fp) # tells you what files are in the filepath directory
docs <- Corpus(DirSource(fp))
class(docs)
str(docs)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, ":")
docs <- tm_map(docs, toSpace, "창")
docs <- tm_map(docs, toSpace, "찾")
docs <- tm_map(docs, toSpace, "%")
docs <- tm_map(docs, removeWords, c("url"))
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
class(docs)
docs.dtm <- DocumentTermMatrix(docs)
docs.dtm # 34% sparsity, 2 documents, 28,127 terms
docs.dtm.df = as.data.frame(as.matrix(docs.dtm))
docs.dtm.df.t <- t(docs.dtm.df)
cor(docs.dtm.df.t)
chitable <- table(docs.dtm.df.t)
chisq.test(chitable)
docs.dtm.s <- removeSparseTerms(docs.dtm, 0.20) # This makes a matrix that is 20% empty space, maximum.
docs.dtm.s # 9,159 terms, 2 docs, 0% sparse
terms <-DocumentTermMatrix(docs,control=list(weighting=function(x) weightTfIdf(x,normalize=FALSE)))
terms # 2 docs, 28,127 terms, 66% sparse
terms.df <- as.data.frame(as.matrix(terms))
terms.df.t <- t(terms.df)
findFreqTerms(terms[1,], lowfreq=40)
findFreqTerms(terms[2,], lowfreq=40)
summing <- function(x) x/sum(x, na.rm=T)
freq.docs <- colSums(docs.dtm)
freq.docs <- colSums(docs.dtm.df)
freq.docs <- sort(freq.docs, decreasing = TRUE)
head(freq.docs, 30) # gives top 30 most used words
count.docs <- rowSums(docs.dtm.df)
count.docs # 1,400,486 words
wordcloud(names(freq.docs), freq.docs, min.freq=600, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq.docs), freq.docs, min.freq=400, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq.docs), freq.docs, min.freq=300, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq.docs), freq.docs, min.freq=300, colors=brewer.pal(8, "Dark2"), random.order = FALSE)
freq.docs.m <- colSums(docs.dtm.df[1,])
freq.docs.m <- sort(freq.docs.m, decreasing = TRUE)
head(freq.docs.m, 30) # gives top 30 most used words
freq.docs.m <- colSums(docs.dtm.df[1])
freq.docs.m <- sort(freq.docs.m, decreasing = TRUE)
head(freq.docs.m, 30) # gives top 30 most used words
getwd()
